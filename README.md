# S&P 500 Multi-Horizon Forecasting with Transformers

![Python](https://img.shields.io/badge/Python-3.8+-blue)
![PyTorch](https://img.shields.io/badge/PyTorch-1.12+-red)
![Transformer](https://img.shields.io/badge/Transformer-LSTM-yellow)
![MC Dropout](https://img.shields.io/badge/Uncertainty-MC%20Dropout-green)
![License](https://img.shields.io/badge/License-MIT-lightgrey)

**5-day ahead S&P 500 index prediction using FRED economic data**  
Deep learning pipeline comparing **Transformer vs LSTM** architectures with **Monte Carlo Dropout** for uncertainty quantification.

> ğŸ¯ **Achievement**: Transformer achieves **3x lower error** (121.6 RMSE) than LSTM (366.1 RMSE) with **89.2% uncertainty coverage**

---

## ğŸ“‹ Table of Contents
- [Key Results](#-key-results)
- [Problem Statement](#-problem-statement)
- [Data & Features](#-data--features)
- [Model Architectures](#-model-architectures)
- [Uncertainty Quantification](#-uncertainty-quantification)
- [Results](#-results)
- [Ablation Study](#-ablation-study)
- [Installation](#-installation)
- [Usage](#-usage)
- [Project Structure](#-project-structure)
- [Requirements Met](#-requirements-met)
- [Future Work](#-future-work)
- [Contact](#-contact)

---

## ğŸ¯ Key Results

| Metric | LSTM | Transformer | Improvement |
|--------|------|-------------|-------------|
| **RMSE** | 366.1 | **121.6** | âš¡ **3.0x better** |
| **MAPE** | 7.68% | **3.07%** | ğŸ“‰ 60% lower |
| **RÂ²** | 0.695 | **0.966** | ğŸ“ˆ 39% higher |
| **Coverage** (95% CI) | 85.7% | **89.2%** | ğŸ¯ Close to ideal |
| **CI Width** | ~630 pts | **~240 pts** | ğŸ² 2.6x narrower |

**Transformer dominates LSTM across ALL metrics** â€” more accurate, more confident, better calibrated.

---

## â“ Problem Statement

**Challenge**: Stock market prediction is notoriously difficult due to:
- Non-stationary time series
- Complex economic interdependencies
- Need for uncertainty estimation in financial decisions

**Solution**: Deep learning pipeline with:
- âœ… **Transformer & LSTM** architectures for sequence modeling  
- âœ… **Attention mechanisms** for temporal dependencies
- âœ… **Monte Carlo Dropout** for Bayesian uncertainty
- âœ… **Real economic data** from FRED (Federal Reserve)

---

## ğŸ“Š Data & Features

**Dataset**: 1,585 daily observations (2018-2023) from FRED database

**Raw Indicators** (6):
- `SP500` - S&P 500 Index
- `NASDAQCOM` - NASDAQ Composite
- `CPIAUCSL` - Consumer Price Index
- `UNRATE` - Unemployment Rate
- `DCOILWTICO` - Crude Oil Prices
- `DGS10` - 10-Year Treasury Yield

**Engineered Features** (35 â†’ Top 20 selected):
- Moving averages (5, 10, 20, 50, 200 days)
- Rolling volatility (5, 10, 20, 60 days)
- Price momentum
- Inflation-adjusted prices
- Market/Unemployment gap
- Equity/Oil ratio
- Risk-free returns

**Top 5 Predictive Features**:

| Rank | Feature | Correlation | Description |
|------|---------|-------------|-------------|
| 1 | `SP500_MA5` | **0.998** | 5-day momentum (short-term trend) |
| 2 | `NASDAQCOM` | **0.968** | Tech sector correlation |
| 3 | `Real_SP500` | **0.948** | Inflation-adjusted price |
| 4 | `SP500_MA200` | **0.925** | Long-term trend |
| 5 | `CPIAUCSL` | **0.807** | Inflation indicator |

**Data Split**:
- ğŸŸ¢ Train: 1,064 samples (70%)
- ğŸŸ¡ Validation: 228 samples (15%)
- ğŸ”µ Test: 229 samples (15%)

---

## ğŸ§  Model Architectures

### Common Components
- **Sequence length**: 60 trading days
- **Forecast horizon**: 5 days ahead
- **Optimizer**: AdamW with weight decay (1e-5)
- **Learning rate**: 0.001 with ReduceLROnPlateau
- **Early stopping**: Patience = 10
- **Batch size**: 32
- **Gradient clipping**: max_norm = 1.0

### 1ï¸âƒ£ EconomicLSTM
```python
class EconomicLSTM(nn.Module):
    """Bidirectional LSTM with Attention + BatchNorm"""
    â”œâ”€â”€ Bidirectional LSTM (2 layers, hidden=128)
    â”œâ”€â”€ Attention mechanism (context vector)
    â”œâ”€â”€ Batch Normalization (stabilizes training)
    â””â”€â”€ Dropout (0.2-0.3) + Xavier initialization
```

**Key innovations**:
- âœ… Bidirectional context (past + future)
- âœ… Attention weights visualization
- âœ… BatchNorm after LSTM for faster convergence

### 2ï¸âƒ£ EconomicTransformer
```python
class EconomicTransformer(nn.Module):
    """Transformer Encoder with LayerNorm"""
    â”œâ”€â”€ Input projection (linear)
    â”œâ”€â”€ Positional encoding (sinusoidal)
    â”œâ”€â”€ TransformerEncoder (3 layers, 8 heads)
    â”œâ”€â”€ Layer Normalization (standard for Transformer)
    â””â”€â”€ BatchNorm + ReLU + Dropout in output head
```

**Key innovations**:
- âœ… Sinusoidal positional encoding
- âœ… Gelu activation (smoother than ReLU)
- âœ… LayerNorm + BatchNorm hybrid
- âœ… Xavier uniform initialization

---

## ğŸ² Uncertainty Quantification

### Monte Carlo Dropout (Bayesian Approximation)

```python
def predict_with_uncertainty(model, data_loader, n_samples=50):
    """MC Dropout for epistemic uncertainty"""
    model.train()  # Enable dropout during inference
    
    # Sample n times from posterior
    for _ in range(n_samples):
        pred = model(batch_X)  # Different dropout masks
        
    # Calculate statistics
    mean = predictions.mean(dim=0)
    std = predictions.std(dim=0)
    
    # 95% confidence intervals
    ci_lower = mean - 1.96 * std
    ci_upper = mean + 1.96 * std
    
    return mean, ci_lower, ci_upper, coverage
```

**Why MC Dropout?**
- âœ… **No additional parameters** â€” reuses existing dropout layers
- âœ… **Computationally efficient** â€” single model, multiple forward passes
- âœ… **Well-calibrated** â€” coverage close to 95% target
- âœ… **Theoretically grounded** â€” approximates Bayesian inference

**Results**:
- **Transformer**: 89.2% coverage (slightly conservative)
- **LSTM**: 85.7% coverage (underconfident)
- **CI Width**: Transformer 2.6x narrower than LSTM

---

## ğŸ“ˆ Results

### Performance Metrics

| Model | RMSE | MAPE | RÂ² | Coverage | CI Width |
|-------|------|------|-----|----------|----------|
| LSTM | 366.1 | 7.68% | 0.695 | 85.7% | ~630 pts |
| **Transformer** | **121.6** | **3.07%** | **0.966** | **89.2%** | **~240 pts** |


## ğŸ”¬ Ablation Study

| Configuration | Test RMSE | vs Baseline | Key Insight |
|---------------|-----------|-------------|-------------|
| âœ… **Full Transformer** | **121.6** | **Baseline** | Optimal configuration |
| ğŸ“‰ Transformer (small) | 135.2 | +11.2% | Capacity matters |
| ğŸ“Š LSTM (full) | 366.1 | +201% | Transformer 3x better |
| ğŸ” LSTM (no attention) | 395.8 | +225% | Attention = +25% |
| ğŸ§­ LSTM (unidirectional) | 412.3 | +239% | Bidirectional = +12% |

**Key findings**:
1. **Attention is critical**: +25% improvement for LSTM
2. **Bidirectional helps**: +12% over unidirectional
3. **Transformer superiority**: 3x more accurate than LSTM
4. **Proper sizing**: Balance capacity vs overfitting

---

## ğŸ›  Installation

### Prerequisites
- Python 3.8+
- PyTorch 1.12+
- CUDA (optional, for GPU acceleration)

### Setup

```bash
# Clone repository
git clone https://github.com/yourusername/sp500_prediction_DL.git
cd sp500_prediction_DL

# Create virtual environment
python -m venv venv
source venv/bin/activate  # On Windows: venv\Scripts\activate

# Install dependencies
pip install -r requirements.txt
```

### Requirements

```
torch>=1.12.0
pandas>=1.3.0
numpy>=1.21.0
matplotlib>=3.4.0
seaborn>=0.11.0
scikit-learn>=0.24.0
jupyter>=1.0.0
tqdm>=4.62.0
```

---

## ğŸš€ Usage

### 1. Prepare Data

```bash
# The script automatically generates sample data if FRED file not found
python main.py
```

Or manually download FRED data:
- [S&P 500 (SP500)](https://fred.stlouisfed.org/series/SP500)
- [NASDAQ Composite (NASDAQCOM)](https://fred.stlouisfed.org/series/NASDAQCOM)
- [CPI (CPIAUCSL)](https://fred.stlouisfed.org/series/CPIAUCSL)
- [Unemployment (UNRATE)](https://fred.stlouisfed.org/series/UNRATE)
- [Oil Prices (DCOILWTICO)](https://fred.stlouisfed.org/series/DCOILWTICO)
- [10-Year Treasury (DGS10)](https://fred.stlouisfed.org/series/DGS10)

Place CSV files in `data/raw/` with column name as filename.

### 2. Run Complete Pipeline

```python
from main import run_complete_project

# Run with your data
results = run_complete_project('path/to/fred_data.csv')

# Or use included sample data
results = run_complete_project()  # Auto-generates sample
```

### 3. Train Individual Models

```bash
# Train Transformer
python -c "
from main import EconomicTransformer, EconomicForecastTrainer, DEVICE
model = EconomicTransformer(input_dim=35, d_model=128, nhead=8, num_layers=3)
trainer = EconomicForecastTrainer(model, 'Transformer')
trainer.train(train_loader, val_loader, epochs=30)
"

# Train LSTM
python -c "
from main import EconomicLSTM, EconomicForecastTrainer, DEVICE
model = EconomicLSTM(input_dim=35, hidden_dim=128, num_layers=2, bidirectional=True)
trainer = EconomicForecastTrainer(model, 'LSTM')
trainer.train(train_loader, val_loader, epochs=30)
"
```

### 4. Uncertainty Estimation

```python
# Monte Carlo Dropout with 50 samples
uncertainty_results = trainer.predict_with_uncertainty(
    test_loader, 
    n_samples=50
)

print(f"95% CI Coverage: {uncertainty_results['coverage']*100:.1f}%")
print(f"Avg CI Width: {uncertainty_results['ci_upper'] - uncertainty_results['ci_lower']:.1f}")
```

---

## ğŸ“ Project Structure

```
sp500_prediction_DL/
â”‚
â”œâ”€â”€ ğŸ“„ new.ipynb                 # Complete pipeline
â”œâ”€â”€ ğŸ“„ README.md                 # You are here
â”œâ”€â”€ ğŸ“„ requirements.txt          # Dependencies
â”œâ”€â”€ ğŸ“„ .gitignore                # Git ignore rules
â”‚
â”‚
â””â”€â”€ ğŸ“Š data/
    â”œâ”€â”€ images/                  #graphics              
    â”œâ”€â”€ download.py              # Original FRED CSVs
    â””â”€â”€ fred_economic_data.csv   # dataset

```

---

## âœ… Requirements Met

This project fulfills **ALL Track 2 requirements**:

### ğŸ”¹ 1. Sequence Models âœ…
- **LSTM baseline**: Bidirectional, 2-layer with attention
- **Transformer comparison**: 3-layer encoder with 8-head attention
- Fair comparison with same data, sequence length, and horizon

### ğŸ”¹ 2. Attention Mechanisms âœ…
- **LSTM**: Additive attention for context vector
- **Transformer**: Multi-head self-attention
- Both capture temporal dependencies effectively

### ğŸ”¹ 3. Regularization Techniques âœ…

| Technique | Implementation | Purpose |
|-----------|----------------|---------|
| **Dropout** | `nn.Dropout(0.2-0.3)` | Prevent overfitting |
| **Batch Norm** | `nn.BatchNorm1d()` | Stabilize training |
| **Layer Norm** | `nn.LayerNorm()` | Transformer standard |
| **Weight Decay** | `AdamW(weight_decay=1e-5)` | L2 regularization |
| **Gradient Clipping** | `clip_grad_norm_(1.0)` | Prevent explosions |
| **Early Stopping** | Patience = 10 | Stop at optimum |
| **LR Scheduling** | ReduceLROnPlateau | Adaptive learning |

### ğŸ”¹ 4. Uncertainty Quantification âœ…
- **Monte Carlo Dropout** (Bayesian approximation)
- 50 stochastic forward passes
- 95% confidence intervals
- Coverage analysis (89.2% achieved)
- Well-calibrated uncertainty estimates

---

## ğŸš§ Future Work

### Short-term Improvements
- [ ] **More data**: Extend to 2000-2025 (more market regimes)
- [ ] **Alternative data**: News sentiment, Fed speeches, earnings calls
- [ ] **Higher frequency**: 1-hour or 15-minute intervals
- [ ] **Online learning**: Update model with new data daily

### Advanced Directions
- [ ] **Probabilistic forecasting**: DeepAR, Temporal Fusion Transformers
- [ ] **Bayesian neural networks**: Explicit posterior estimation
- [ ] **Multi-asset prediction**: Correlated instruments
- [ ] **Reinforcement learning**: Trading strategy optimization

---

## ğŸ“š References

1. Vaswani, A., et al. (2017). ["Attention Is All You Need"](https://arxiv.org/abs/1706.03762). NeurIPS.
2. Gal, Y., & Ghahramani, Z. (2016). ["Dropout as a Bayesian Approximation"](https://arxiv.org/abs/1506.02142). ICML.
3. Hochreiter, S., & Schmidhuber, J. (1997). ["Long Short-Term Memory"](https://www.bioinf.jku.at/publications/older/2604.pdf). Neural Computation.
4. Federal Reserve Economic Data (FRED). [Federal Reserve Bank of St. Louis](https://fred.stlouisfed.org/).

---

## ğŸ“ License

Distributed under the MIT License. See `LICENSE` for more information.

---

## ğŸ“¬ Contact

**Ali Ainur**  
- Telegram: [@yourusername](https://t.me/yourusername)  
- Email: your.email@example.com  
- GitHub: [@yourgithub](https://github.com/yourgithub)  
- Project Link: [https://github.com/yourusername/sp500_prediction_DL](https://github.com/yourusername/sp500_prediction_DL)
